{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJp_eMOfdAJQ",
        "outputId": "cc89d0cc-93c2-4f9f-eeb8-3ab2e07904cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn keras\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install pandas numpy scikit-learn keras\n",
        "\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, Dropout, BatchNormalization\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import traceback\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "log_dir = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), 'logs')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "logging.basicConfig(\n",
        "    filename=os.path.join(log_dir, 'app.log'),\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s]: %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# Suppress deprecation warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "'''\n",
        "# Load the dataset\n",
        "data_directory = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), 'data')\n",
        "dataset_path = os.path.join(data_directory, 'ner_dataset.csv')\n",
        "if os.path.exists(dataset_path):\n",
        "    df_ner = pd.read_csv(dataset_path, encoding=\"latin1\")\n",
        "else:\n",
        "    logging.error(f\"Dataset not found at: {dataset_path}\")\n",
        "'''\n",
        "df_ner = pd.read_csv('ner_dataset.csv', encoding=\"latin1\")\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_ner = df_ner.drop(columns=[\"POS\"])\n",
        "\n",
        "print(df_ner.head())\n",
        "\n",
        "# we'll preprocess the data by creating a function to transform the dataset into sentences and corresponding labels:\n",
        "\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocesses the NER dataset into sentences and corresponding labels.\n",
        "\n",
        "    Args:\n",
        "    data: pandas DataFrame containing the NER dataset\n",
        "\n",
        "    Returns:\n",
        "    sentences: list of lists containing words for each sentence\n",
        "    labels: list of lists containing NER labels for each sentence\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_sentence = []\n",
        "    current_labels = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        # If it's the start of a new sentence\n",
        "        if pd.isnull(row['Sentence #']):\n",
        "            sentences.append(current_sentence)\n",
        "            labels.append(current_labels)\n",
        "            current_sentence = []\n",
        "            current_labels = []\n",
        "        else:\n",
        "            current_sentence.append(row['Word'])\n",
        "            current_labels.append(row['Tag'])\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "sentences, labels = preprocess_data(df_ner)\n",
        "\n",
        "# Now, let's split the dataset into train, validation, and test sets:\n",
        "\n",
        "# Split the data into train and test sets (80% train, 20% test)\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the train set into train and validation sets (80% train, 20% validation)\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_sentences, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# With the data preprocessed and split, we can now move on to building the baseline model. We'll use a simple LSTM-based model for this:\n",
        "\n",
        "# Tokenize words and labels\n",
        "words = list(set(df_ner[\"Word\"].values))\n",
        "n_words = len(words)\n",
        "\n",
        "tags = list(set(df_ner[\"Tag\"].values))\n",
        "n_tags = len(tags)\n",
        "\n",
        "# Create mappings for words and tags\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "\n",
        "# Padding sequences\n",
        "max_len = 50\n",
        "X_train = [[word2idx[w] for w in s] for s in train_sentences]\n",
        "X_train = pad_sequences(maxlen=max_len, sequences=X_train, padding=\"post\", value=n_words-1)\n",
        "\n",
        "y_train = [[tag2idx[t] for t in l] for l in train_labels]\n",
        "y_train = pad_sequences(maxlen=max_len, sequences=y_train, padding=\"post\", value=tag2idx[\"O\"])\n",
        "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]\n",
        "\n",
        "try:\n",
        "    # Define the model architecture\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim=n_words, output_dim=10, input_length=max_len))\n",
        "    model.add(LSTM(units=20, return_sequences=True, recurrent_dropout=0.1))\n",
        "    model.add(Dropout(0.5))  # Added dropout for regularization\n",
        "    model.add(Dense(n_tags, activation=\"softmax\"))\n",
        "\n",
        "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    # Define early stopping\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "\n",
        "    # Define model checkpointing\n",
        "    checkpoint = ModelCheckpoint('model.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(X_train, np.array(y_train), batch_size=32, epochs=5, validation_split=0.1, verbose=1, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "except Exception as e:\n",
        "    logging.error(\"Exception occurred\", exc_info=True)\n",
        "    traceback.print_exc()\n",
        "\n",
        "#This code sets up a basic LSTM-based model for NER, tokenizes words and labels, pads sequences, defines the model architecture, compiles the model, and finally trains it.\n",
        "\n",
        "# Prepare the test data\n",
        "X_test = [[word2idx.get(w, n_words-1) for w in s] for s in test_sentences]\n",
        "X_test = pad_sequences(maxlen=max_len, sequences=X_test, padding=\"post\", value=n_words-1)\n",
        "\n",
        "y_test = [[tag2idx.get(t, tag2idx[\"O\"]) for t in l] for l in test_labels]\n",
        "y_test = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\", value=tag2idx[\"O\"])\n",
        "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert the index to tag\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "\n",
        "def pred2label(pred):\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i].replace(\"PAD\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "\n",
        "pred_labels = pred2label(y_pred)\n",
        "test_labels = pred2label(y_test)\n",
        "\n",
        "# Flatten the lists of labels and predictions\n",
        "flat_test_labels = [label for sublist in test_labels for label in sublist]\n",
        "flat_pred_labels = [label for sublist in pred_labels for label in sublist]\n",
        "\n",
        "# Print the classification report\n",
        "#print(classification_report(test_labels, pred_labels))\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(flat_test_labels, flat_pred_labels, average='weighted')\n",
        "\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSxz3kSKdGUe",
        "outputId": "bb1ce855-ee5a-40b3-a69c-ee0823f32e84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Sentence #           Word Tag\n",
            "0  Sentence: 1      Thousands   O\n",
            "1          NaN             of   O\n",
            "2          NaN  demonstrators   O\n",
            "3          NaN           have   O\n",
            "4          NaN        marched   O\n",
            "Epoch 1/5\n",
            "18012/18012 [==============================] - ETA: 0s - loss: 0.0106 - accuracy: 0.9995\n",
            "Epoch 1: val_loss improved from inf to 0.00034, saving model to model.h5\n",
            "18012/18012 [==============================] - 578s 32ms/step - loss: 0.0106 - accuracy: 0.9995 - val_loss: 3.3726e-04 - val_accuracy: 0.9999\n",
            "Epoch 2/5\n",
            "    1/18012 [..............................] - ETA: 8:26 - loss: 4.9405e-06 - accuracy: 1.0000"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18011/18012 [============================>.] - ETA: 0s - loss: 2.9212e-04 - accuracy: 0.9999\n",
            "Epoch 2: val_loss improved from 0.00034 to 0.00026, saving model to model.h5\n",
            "18012/18012 [==============================] - 562s 31ms/step - loss: 2.9212e-04 - accuracy: 0.9999 - val_loss: 2.5955e-04 - val_accuracy: 0.9999\n",
            "Epoch 3/5\n",
            "18011/18012 [============================>.] - ETA: 0s - loss: 2.2470e-04 - accuracy: 1.0000\n",
            "Epoch 3: val_loss improved from 0.00026 to 0.00026, saving model to model.h5\n",
            "18012/18012 [==============================] - 560s 31ms/step - loss: 2.2470e-04 - accuracy: 1.0000 - val_loss: 2.5568e-04 - val_accuracy: 0.9999\n",
            "Epoch 4/5\n",
            "18011/18012 [============================>.] - ETA: 0s - loss: 2.0048e-04 - accuracy: 1.0000\n",
            "Epoch 4: val_loss did not improve from 0.00026\n",
            "18012/18012 [==============================] - 562s 31ms/step - loss: 2.0048e-04 - accuracy: 1.0000 - val_loss: 2.5976e-04 - val_accuracy: 0.9999\n",
            "Epoch 5/5\n",
            "18012/18012 [==============================] - ETA: 0s - loss: 1.8783e-04 - accuracy: 1.0000\n",
            "Epoch 5: val_loss improved from 0.00026 to 0.00025, saving model to model.h5\n",
            "18012/18012 [==============================] - 601s 33ms/step - loss: 1.8783e-04 - accuracy: 1.0000 - val_loss: 2.5461e-04 - val_accuracy: 0.9999\n",
            "6254/6254 [==============================] - 47s 7ms/step\n",
            "F1 Score: 0.9999191413875568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ie1uHIW6tBdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}