{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8413454,"sourceType":"datasetVersion","datasetId":5007705}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-15T08:05:47.768643Z","iopub.execute_input":"2024-05-15T08:05:47.769506Z","iopub.status.idle":"2024-05-15T08:05:47.786367Z","shell.execute_reply.started":"2024-05-15T08:05:47.769456Z","shell.execute_reply":"2024-05-15T08:05:47.785328Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"/kaggle/input/ner-data/ner_dataset.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install pandas numpy scikit-learn keras tensorflow thinc==8.2.3","metadata":{"execution":{"iopub.status.busy":"2024-05-15T08:05:48.403864Z","iopub.execute_input":"2024-05-15T08:05:48.404283Z","iopub.status.idle":"2024-05-15T08:06:02.064768Z","shell.execute_reply.started":"2024-05-15T08:05:48.404227Z","shell.execute_reply":"2024-05-15T08:06:02.063502Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (2.1.4)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.4)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (1.2.2)\nRequirement already satisfied: keras in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: tensorflow in /opt/conda/lib/python3.10/site-packages (2.15.0)\nRequirement already satisfied: thinc==8.2.3 in /opt/conda/lib/python3.10/site-packages (8.2.3)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (0.7.10)\nRequirement already satisfied: murmurhash<1.1.0,>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (1.0.10)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (2.0.8)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (3.0.9)\nRequirement already satisfied: wasabi<1.2.0,>=0.8.1 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (1.1.2)\nRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (2.4.8)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.4 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (2.0.10)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (0.1.4)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (69.0.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (2.5.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from thinc==8.2.3) (21.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas) (2023.4)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (1.4.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn) (3.2.0)\nRequirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.4.0)\nRequirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.6.3)\nRequirement already satisfied: flatbuffers>=23.5.26 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (23.5.26)\nRequirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.5.4)\nRequirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.10.0)\nRequirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (16.0.6)\nRequirement already satisfied: ml-dtypes~=0.2.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.2.0)\nRequirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.3.0)\nRequirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.20.3)\nRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\nRequirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.4.0)\nRequirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.9.0)\nRequirement already satisfied: wrapt<1.15,>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.14.1)\nRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (0.35.0)\nRequirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.51.1)\nRequirement already satisfied: tensorboard<2.16,>=2.15 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.1)\nRequirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (2.15.0)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->thinc==8.2.3) (3.1.1)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.3) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->thinc==8.2.3) (2.14.6)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.26.1)\nRequirement already satisfied: google-auth-oauthlib<2,>=0.5 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.2)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# !pip install pandas numpy scikit-learn keras tensorflow thinc==8.2.3\n# Tested on GPU P100 (Kaggle Kernel)\n\nimport logging\nimport os\nimport random\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport spacy\nfrom spacy.util import minibatch, compounding\nfrom spacy.training import Example\nfrom spacy.scorer import Scorer\nfrom thinc.api import require_gpu # 3rd party API\n\n# Use GPU if available\nrequire_gpu()\n\n# Determine the parent directory of the current working directory\nparent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n\n# Create the log directory if it doesn't exist\nlog_dir = os.path.join(parent_dir, 'logs')\nos.makedirs(log_dir, exist_ok=True)\n\n# Configure the logging settings\nlogging.basicConfig(\n    filename=os.path.join(log_dir, 'app.log'),  # Log file path\n    level=logging.INFO,  # Set the desired log level\n    format='%(asctime)s [%(levelname)s]: %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\n\n# Enable Console Logging:\nconsole_handler = logging.StreamHandler()\nconsole_handler.setLevel(logging.INFO)\nformatter = logging.Formatter('%(asctime)s [%(levelname)s]: %(message)s')\nconsole_handler.setFormatter(formatter)\nlogging.getLogger().addHandler(console_handler)\n\ndf_ner = pd.read_csv('/kaggle/input/ner-data/ner_dataset.csv', encoding=\"latin1\")\n\n# Drop POS since its not needed:\ndf_ner = df_ner.drop(columns=[\"POS\"])\n\n# Display the first few rows of the dataset\nprint(df_ner.head())\n\n# Preprocess the data\ndf_ner = df_ner.ffill()\n#df_ner = df_ner.fillna(method='ffill')\nwords = list(set(df_ner['Word'].values))\nn_words = len(words)\n\nclass SentenceGetter(object):\n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s['Word'].tolist(), s['Tag'].tolist())]\n        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n\n        \ngetter = SentenceGetter(df_ner)\nsentences = getter.sentences\n\n# Split the data into train and test\ntrain_sentences, test_sentences = train_test_split(sentences, test_size=0.2)\n\n'''\n# Create blank Language class\nnlp = spacy.blank('en', disable=['tagger', 'parser'])\n\n# Create the built-in pipeline components and add them to the pipeline\nner = nlp.create_pipe('ner')\nnlp.add_pipe('ner')\n'''\n\n# Create blank Language class\nnlp = spacy.blank('en')\n\n# Disable tagger and parser\n#nlp.disable_pipes('tagger', 'parser')\n\n# Create the built-in pipeline components and add them to the pipeline\nner = nlp.create_pipe('ner')\nnlp.add_pipe('ner')\n\nTRAIN_DATA = []\nfor sentence in train_sentences:\n    ents = []\n    start = 0\n    for word, tag in sentence:\n        end = start + len(word)\n        if tag != 'O':\n            ents.append((start, end, tag))\n        start = end + 1  # +1 for the space between words\n    sentence = \" \".join(word for word, tag in sentence)\n    TRAIN_DATA.append((sentence, {\"entities\": ents}))\n\n# Only train NER\noptimizer = nlp.begin_training()\nfor itn in range(10):\n    print(\"Starting iteration \" + str(itn))\n    random.shuffle(TRAIN_DATA)\n    losses = {}\n    #batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n    batches = minibatch(TRAIN_DATA, size=256)\n    for batch in batches:\n        texts, annotations = zip(*batch)\n        example = [Example.from_dict(nlp.make_doc(text), annotations) for text, annotations in batch]\n        nlp.update(example, drop=0.5, sgd=optimizer, losses=losses)\n    print(\"Model losses are:\")\n    print(losses)\n\n\n# Function to evaluate the model\ndef evaluate(ner_model, examples):\n    scorer = Scorer(ner_model)\n    scores = {}\n    for input_, annot in examples:\n        doc_gold_text = ner_model.make_doc(input_)\n        example = Example.from_dict(doc_gold_text, annot)\n        scores = scorer.score([example])  # Pass a list of Example objects\n    return scores\n\n# Test the trained model\ntest_data = []\nfor test_sentence in test_sentences:\n    sentence = []\n    entities = []\n    start = 0\n    for word_tag_pair in test_sentence:\n        word, tag = word_tag_pair\n        sentence.append(word)\n        if tag != 'O':\n            end = start + len(word)\n            entities.append((start, end, tag))\n        start += len(word) + 1\n    test_data.append((' '.join(sentence), {\"entities\": entities}))\n\n# Get the evaluation results\nresults = evaluate(nlp, test_data)\nprint(\"Test Results are :\")\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T08:06:02.067216Z","iopub.execute_input":"2024-05-15T08:06:02.067601Z","iopub.status.idle":"2024-05-15T08:18:24.919399Z","shell.execute_reply.started":"2024-05-15T08:06:02.067546Z","shell.execute_reply":"2024-05-15T08:18:24.918393Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"    Sentence #           Word Tag\n0  Sentence: 1      Thousands   O\n1          NaN             of   O\n2          NaN  demonstrators   O\n3          NaN           have   O\n4          NaN        marched   O\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/2138609709.py:60: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n  self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n[2024-05-15 08:06:10,071] [INFO] Created vocabulary\n2024-05-15 08:06:10,071 [INFO]: Created vocabulary\n2024-05-15 08:06:10,071 [INFO]: Created vocabulary\n2024-05-15 08:06:10,071 [INFO]: Created vocabulary\n2024-05-15 08:06:10,071 [INFO]: Created vocabulary\n[2024-05-15 08:06:10,074] [INFO] Finished initializing nlp object\n2024-05-15 08:06:10,074 [INFO]: Finished initializing nlp object\n2024-05-15 08:06:10,074 [INFO]: Finished initializing nlp object\n2024-05-15 08:06:10,074 [INFO]: Finished initializing nlp object\n2024-05-15 08:06:10,074 [INFO]: Finished initializing nlp object\n","output_type":"stream"},{"name":"stdout","text":"Starting iteration 0\nModel losses are:\n{'ner': 176080.44772854648}\nStarting iteration 1\nModel losses are:\n{'ner': 76324.38909603568}\nStarting iteration 2\nModel losses are:\n{'ner': 62452.727324540225}\nStarting iteration 3\nModel losses are:\n{'ner': 56849.39482926891}\nStarting iteration 4\nModel losses are:\n{'ner': 53450.78118767118}\nStarting iteration 5\nModel losses are:\n{'ner': 50669.99966472123}\nStarting iteration 6\nModel losses are:\n{'ner': 49016.06655813923}\nStarting iteration 7\nModel losses are:\n{'ner': 47685.03433937278}\nStarting iteration 8\nModel losses are:\n{'ner': 46412.75562675036}\nStarting iteration 9\nModel losses are:\n{'ner': 45374.946218410085}\nTest Results are :\n{'token_acc': 1.0, 'token_p': 1.0, 'token_r': 1.0, 'token_f': 1.0, 'ents_p': 0.0, 'ents_r': 0.0, 'ents_f': 0.0, 'ents_per_type': {'B-gpe': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'B-org': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'B-per': {'p': 0.0, 'r': 0.0, 'f': 0.0}, 'I-per': {'p': 0.0, 'r': 0.0, 'f': 0.0}}}\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}