{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJp_eMOfdAJQ",
        "outputId": "16e50a32-09c2-4d62-fa5e-3671949abe4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pandas numpy scikit-learn keras tensorflow\n",
        "# Used GPU T4 (Colab Kernel)\n",
        "\n",
        "# Import necessary libraries\n",
        "import logging\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, f1_score\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Embedding, Dense, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import plot_model\n",
        "import traceback\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "\n",
        "# Configure logging\n",
        "log_dir = os.path.join(os.path.abspath(os.path.join(os.getcwd(), os.pardir)), 'logs')\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "logging.basicConfig(\n",
        "    filename=os.path.join(log_dir, 'app.log'),\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s [%(levelname)s]: %(message)s',\n",
        "    datefmt='%Y-%m-%d %H:%M:%S'\n",
        ")\n",
        "\n",
        "# Check if GPU is available\n",
        "if tf.config.list_physical_devices('GPU'):\n",
        "    print(\"GPU is available!!\")\n",
        "    device = '/gpu:0'\n",
        "else:\n",
        "    print(\"GPU not available, using CPU instead!\")\n",
        "    device = '/cpu:0'\n",
        "\n",
        "# Read the dataset\n",
        "data_directory = os.getcwd()\n",
        "dataset_path = os.path.join(data_directory, 'ner_dataset.csv')\n",
        "if os.path.exists(dataset_path):\n",
        "    df_ner = pd.read_csv(dataset_path, encoding=\"latin1\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"Dataset not found at: {dataset_path}\")\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_ner = df_ner.drop(columns=[\"POS\"])\n",
        "\n",
        "# Function to preprocess the data into sentences and corresponding labels\n",
        "def preprocess_data(data):\n",
        "    \"\"\"\n",
        "    Preprocesses the NER dataset into sentences and corresponding labels.\n",
        "\n",
        "    Args:\n",
        "    data: pandas DataFrame containing the NER dataset\n",
        "\n",
        "    Returns:\n",
        "    sentences: list of lists containing words for each sentence\n",
        "    labels: list of lists containing NER labels for each sentence\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    current_sentence = []\n",
        "    current_labels = []\n",
        "\n",
        "    for index, row in data.iterrows():\n",
        "        if pd.isnull(row['Sentence #']):\n",
        "            if current_sentence:  # Check if the sentence is not empty\n",
        "                sentences.append(current_sentence)\n",
        "                labels.append(current_labels)\n",
        "            current_sentence = []\n",
        "            current_labels = []\n",
        "        else:\n",
        "            current_sentence.append(row['Word'])\n",
        "            current_labels.append(row['Tag'])\n",
        "\n",
        "    return sentences, labels\n",
        "\n",
        "# Preprocess the data\n",
        "sentences, labels = preprocess_data(df_ner)\n",
        "\n",
        "# Split the data into train and test sets (80% train, 20% test)\n",
        "train_sentences, test_sentences, train_labels, test_labels = train_test_split(sentences, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Further split the train set into train and validation sets (80% train, 20% validation)\n",
        "train_sentences, val_sentences, train_labels, val_labels = train_test_split(train_sentences, train_labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Tokenize words and labels using only the training data to avoid data leakage\n",
        "words = list(set([word for sentence in train_sentences for word in sentence]))\n",
        "n_words = len(words)\n",
        "\n",
        "tags = list(set(df_ner[\"Tag\"].values))\n",
        "n_tags = len(tags)\n",
        "\n",
        "# Create mappings for words and tags\n",
        "word2idx = {w: i for i, w in enumerate(words)}\n",
        "tag2idx = {t: i for i, t in enumerate(tags)}\n",
        "\n",
        "# Padding sequences\n",
        "max_len = 50\n",
        "X_train = [[word2idx[w] for w in s] for s in train_sentences]\n",
        "X_train = pad_sequences(maxlen=max_len, sequences=X_train, padding=\"post\", value=n_words-1)\n",
        "\n",
        "y_train = [[tag2idx[t] for t in l] for l in train_labels]\n",
        "y_train = pad_sequences(maxlen=max_len, sequences=y_train, padding=\"post\", value=tag2idx[\"O\"])\n",
        "y_train = [to_categorical(i, num_classes=n_tags) for i in y_train]\n",
        "\n",
        "# Function to train the model\n",
        "def train_model(X_train, y_train, n_words, n_tags, device):\n",
        "    \"\"\"\n",
        "    Trains the LSTM model.\n",
        "\n",
        "    Args:\n",
        "    X_train: Training data\n",
        "    y_train: Labels for the training data\n",
        "    n_words: Total number of unique words\n",
        "    n_tags: Total number of unique tags\n",
        "    device: Device to run the model on ('/cpu:0' or '/gpu:0')\n",
        "\n",
        "    Returns:\n",
        "    model: Trained model\n",
        "    \"\"\"\n",
        "    with tf.device(device):\n",
        "\n",
        "        # Define the model architecture\n",
        "        model = Sequential()\n",
        "        model.add(Embedding(input_dim=n_words, output_dim=10, input_length=max_len))\n",
        "        model.add(LSTM(units=20, return_sequences=True, recurrent_dropout=0.1))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(n_tags, activation=\"softmax\"))\n",
        "        # Compile model\n",
        "        model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "        # Define early stopping\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "        # Define model checkpointing\n",
        "        checkpoint = ModelCheckpoint('model.keras', monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "\n",
        "        batch_size = 256 # Larger batch size\n",
        "        # Train the model\n",
        "        model.fit(X_train, np.array(y_train), batch_size=batch_size, epochs=5, validation_split=0.1, verbose=1, callbacks=[early_stopping, checkpoint])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Train the model\n",
        "model = train_model(X_train, y_train, n_words, n_tags, device)\n",
        "\n",
        "# print model summary\n",
        "print(model.summary())\n",
        "plot_model(model)\n",
        "\n",
        "# Prepare the test data\n",
        "X_test = [[word2idx.get(w, n_words-1) for w in s] for s in test_sentences]\n",
        "X_test = pad_sequences(maxlen=max_len, sequences=X_test, padding=\"post\", value=n_words-1)\n",
        "\n",
        "y_test = [[tag2idx.get(t, tag2idx[\"O\"]) for t in l] for l in test_labels]\n",
        "y_test = pad_sequences(maxlen=max_len, sequences=y_test, padding=\"post\", value=tag2idx[\"O\"])\n",
        "y_test = [to_categorical(i, num_classes=n_tags) for i in y_test]\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert the index to tag\n",
        "idx2tag = {i: w for w, i in tag2idx.items()}\n",
        "\n",
        "# Function to convert predictions to labels\n",
        "def pred2label(pred):\n",
        "    \"\"\"\n",
        "    Converts predictions to labels.\n",
        "\n",
        "    Args:\n",
        "    pred: Predictions from the model\n",
        "\n",
        "    Returns:\n",
        "    out: List of predicted labels\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    for pred_i in pred:\n",
        "        out_i = []\n",
        "        for p in pred_i:\n",
        "            p_i = np.argmax(p)\n",
        "            out_i.append(idx2tag[p_i].replace(\"PAD\", \"O\"))\n",
        "        out.append(out_i)\n",
        "    return out\n",
        "\n",
        "# Convert predictions to labels\n",
        "pred_labels = pred2label(y_pred)\n",
        "test_labels = pred2label(y_test)\n",
        "\n",
        "# Flatten the lists of labels and predictions\n",
        "flat_test_labels = [label for sublist in test_labels for label in sublist]\n",
        "flat_pred_labels = [label for sublist in pred_labels for label in sublist]\n",
        "\n",
        "# Calculate the F1 score\n",
        "f1 = f1_score(flat_test_labels, flat_pred_labels, average='weighted')\n",
        "\n",
        "print(f'F1 Score: {f1}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSxz3kSKdGUe",
        "outputId": "2cf13c71-f302-4d52-b9e9-1614b79681ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "108/108 [==============================] - ETA: 0s - loss: 1.2133 - accuracy: 0.9232\n",
            "Epoch 1: val_loss improved from inf to 0.17058, saving model to model.keras\n",
            "108/108 [==============================] - 27s 223ms/step - loss: 1.2133 - accuracy: 0.9232 - val_loss: 0.1706 - val_accuracy: 0.9945\n",
            "Epoch 2/5\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.1541 - accuracy: 0.9941\n",
            "Epoch 2: val_loss improved from 0.17058 to 0.07376, saving model to model.keras\n",
            "108/108 [==============================] - 17s 159ms/step - loss: 0.1541 - accuracy: 0.9941 - val_loss: 0.0738 - val_accuracy: 0.9945\n",
            "Epoch 3/5\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.0800 - accuracy: 0.9943\n",
            "Epoch 3: val_loss improved from 0.07376 to 0.04574, saving model to model.keras\n",
            "108/108 [==============================] - 18s 168ms/step - loss: 0.0800 - accuracy: 0.9943 - val_loss: 0.0457 - val_accuracy: 0.9945\n",
            "Epoch 4/5\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.0537 - accuracy: 0.9943\n",
            "Epoch 4: val_loss improved from 0.04574 to 0.03402, saving model to model.keras\n",
            "108/108 [==============================] - 20s 184ms/step - loss: 0.0537 - accuracy: 0.9943 - val_loss: 0.0340 - val_accuracy: 0.9945\n",
            "Epoch 5/5\n",
            "108/108 [==============================] - ETA: 0s - loss: 0.0417 - accuracy: 0.9943\n",
            "Epoch 5: val_loss improved from 0.03402 to 0.02828, saving model to model.keras\n",
            "108/108 [==============================] - 28s 256ms/step - loss: 0.0417 - accuracy: 0.9943 - val_loss: 0.0283 - val_accuracy: 0.9945\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 50, 10)            29150     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 50, 20)            2480      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 50, 20)            0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50, 17)            357       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31987 (124.95 KB)\n",
            "Trainable params: 31987 (124.95 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n",
            "300/300 [==============================] - 5s 16ms/step\n",
            "F1 Score: 0.9914884881439663\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ie1uHIW6tBdH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}